# Copyright (c) Facebook, Inc. and its affiliates.
import argparse
import glob
import logging
import numpy as np
import os, sys
import tempfile
import time
import warnings
import cv2
import tqdm
from diffusiondet.register_coco import register_coco_instances
from diffusiondet.catalog import MetadataCatalog, DatasetCatalog
from detectron2.config import get_cfg
from detectron2.engine import default_setup
from detectron2.data.detection_utils import read_image
import heapq
from detectron2.engine import DefaultPredictor
import math
from diffusiondet.util.birdview_detection_refiner import BirdviewDetectionRefiner
from diffusiondet.util.utils_3d import _draw_projection_obstacle_to_cam
from diffusiondet.util.object_3d import Object3d
from diffusiondet.util.utils_calib import Calibration
from diffusiondet.dataset_mapper import DatasetMapper

from diffusiondet.predictor import VisualizationDemo
from diffusiondet import add_diffusiondet_config, DiffusionDetWithTTA
from diffusiondet.util.model_ema import add_model_ema_configs, may_build_model_ema, may_get_ema_checkpointer, EMAHook, \
    apply_model_ema_and_restore, EMADetectionCheckpointer
from bisect import bisect, insort
'''
This script allows the user to:
1. Obtain the annotations in KITTI format of one or multiple checkpoints, to be evaluated with an external evaluator like https://github.com/cguindel/eval_kitti
2. Visualize and save the images resulting in both BEV and 3D as well
3. Change the evaluation parameters and kitti_root by arguments
'''

def parse_args():
    parser = argparse.ArgumentParser(description='Validation script for BirdNet+')
    parser.add_argument(
        '--config-file', help="Name of the configuration to use without extension", default='diffdetbev.coco.res50', type=str)
    parser.add_argument(
        '--ann_val', help="Validation file with the annotations in COCO format previously generated by the training script, without extension", default='training_annotations_kitti_carpedcyc_RDHCVPr12', type=str)
    parser.add_argument(
        '--write', help="Write results in KITTI format", default=False, action="store_true")
    parser.add_argument(
        '--img2show', help="Show a fixed number of images, 0 to eliminate the visualization", default=0, type=int)
    parser.add_argument(
        '--save_img', help="Save images showed", default=False, action="store_true")
    parser.add_argument(
        '--eval_chkp', help="Starting from the second half of the checkpoints, the rest will be evaluated with a certain interval specified here, 1 to evaluate all of them", default=1, type=int)
    parser.add_argument(
        '--force_test', help="Name of the checkpoint to extract annotations or evaluate, empty disable this option", default='', type=str)
    parser.add_argument(
        '--score', help="Limitation for lower scores", default=0.01, type=float)
    parser.add_argument(
        '--nms', help="NMS IoU for the overlapping obstacles per class", default=0.3, type=float)
    parser.add_argument(
        '--kitti_root', help="Path of the KITTI dataset", default='/content/DiffusionDetLidar/datasets/bv_kitti/training', type=str)
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)
    return parser.parse_args()



detectron2_root = '/content/DiffusionDetLidar'



# BEV parameters
bvres = 0.05
velodyne_h = 1.73
only_front = True
# BEV images
im_path = os.path.join(detectron2_root,'datasets/bv_kitti/image') 

def _read_imageset_file(path):
    with open(path, 'r') as f:
        lines = f.readlines()
    return [int(line) for line in lines]

# Viewpoint calculation
def getfrombins(cl,bins):
    bin_dist = np.linspace(-math.pi,math.pi,bins+1)
    bin_res = (bin_dist[1]-bin_dist[0])/2.
    bin = [bin_dist[i]-bin_res for i in range(len(bin_dist)-1)][cl] 
    return bin

idclass = { 0:'Car', 1:'Van', 2:'Truck', 3:'Pedestrian', 4:'Person_sitting',  5:'Cyclist', 6:'Tram', 7:'Misc', 8:'DontCare'}
idclass3 = { 0:'Car', 1:'Pedestrian', 2:'Cyclist'}
def catName(category_id,nclass):
    if nclass > 3:
        _idclass = idclass
    elif nclass == 3:
        _idclass = idclass3
    strclass = _idclass.get(category_id, nclass)
    return strclass   

def prepareAnn(lbl, alpha, box, h=-1, w=-1, l=-1, x=-1000, y=-1000, z=-1000, ry=-10, score=None):
    ann = [
       lbl, 
       -1,
       -1,
       alpha,
       box[0],box[1],box[2],box[3],
       h,w,l,
       x,y,z,
       ry
    ]  
    if score is not None:
        ann.append(score)
    strAnn = ' '.join([str(x) for x in ann])
    obj3d = Object3d(strAnn)
     
    return ann, obj3d, strAnn

def box_iou(pred_box, gt_box):
    '''
    Calculate iou for predict box and ground truth box
    Param
         pred_box: predict box coordinate
                   (xmin,ymin,xmax,ymax) format
         gt_box: ground truth box coordinate
                 (xmin,ymin,xmax,ymax) format
    Return
         iou value
    '''
    # get intersection box
    inter_box = [max(pred_box[0], gt_box[0]), max(pred_box[1], gt_box[1]), min(pred_box[2], gt_box[2]), min(pred_box[3], gt_box[3])]
    inter_w = max(0.0, inter_box[2] - inter_box[0] + 1)
    inter_h = max(0.0, inter_box[3] - inter_box[1] + 1)

    # compute overlap (IoU) = area of intersection / area of union
    pred_area = (pred_box[2] - pred_box[0] + 1) * (pred_box[3] - pred_box[1] + 1)
    gt_area = (gt_box[2] - gt_box[0] + 1) * (gt_box[3] - gt_box[1] + 1)
    inter_area = inter_w * inter_h
    union_area = pred_area + gt_area - inter_area
    return 0 if union_area == 0 else float(inter_area) / float(union_area) 
class Closest:
    """Assumes *no* redundant entries - all inputs must be unique"""
    def __init__(self, numlist=None, firstdistance=0):
        if numlist == None:
            numlist=[]
        self.numindexes = dict((val, n) for n, val in enumerate(numlist))
        self.nums = sorted(self.numindexes)
        self.firstdistance = firstdistance

    def append(self, num):
        if num in self.numindexes:
            raise ValueError("Cannot append '%s' it is already used" % str(num))
        self.numindexes[num] = len(self.nums)
        bisect.insort(self.nums, num)

    def rank(self, target):
        rank = bisect.bisect(self.nums, target)
        if rank == 0:
            pass
        elif len(self.nums) == rank:
            rank -= 1
        else:
            dist1 = target - self.nums[rank - 1]
            dist2 = self.nums[rank] - target
            if dist1 < dist2:
                rank -= 1
        return rank

    def closest(self, target):
        try:
            return self.numindexes[self.nums[self.rank(target)]]
        except IndexError:
            return 0

    def distance(self, target):
        rank = self.rank(target)
        try:
            dist = abs(self.nums[rank] - target)
        except IndexError:
            dist = self.firstdistance
        return dist


def get_min(a,b):
  
  np.abs(a-b)

  return np.abs(a - b)

def get_best(myList,myNumber,k):

  return [n for d, n in sorted((abs(x-myNumber), x) for x in myList)[:k]]

  
def k_nearest(k, center, sorted_data):
    'Return *k* members of *sorted_data* nearest to *center*'
    i = bisect(sorted_data, center)
    segment = sorted_data[max(i-k, 0) : i+k]
    return heapq.nsmallest(k, segment, key=lambda x: abs(x - center))

def prepare_for_coco_detection_KITTI(instance, output_folder, filename, write, kitti_calib_path, nclass, vp, bins, vp_res, hwrot, height_training,gt_objs):
    # Extract important information from instance class
    # print('instance',instance)




    boxes  = np.array(instance.get('pred_boxes').tensor)
    # boxes[:,0] = 49+ boxes[:,1]
    # boxes[:,1] = 82 + boxes[:,1]
    # boxes[:,2] = 49 - boxes[:,1]
    # boxes[:,3] = 82- boxes[:,3]
    # print('boexs: ',boxes[:,1])

    # for 
    scores = np.array(instance.get('scores'))
    # gt_bbx = []
    # gt_bbx[0,1,2,3] =  gt_obj.xmin, gt_obj.ymin, gt_obj.xmax, gt_obj.ymax
    # gt_lhw = []
    # gt_lhw[0,1,2] =  gt_obj.length, gt_obj.width, gt_obj.height

    # gt_a = gt_obj.alpha

    # diff_0 = boxes[:,0] - gt_bbx

    # qualified_objs = sum(scores>0.02)

    # print('scores: ',sum(scores>0.02))



    labels = np.array(instance.get('pred_classes'))
    # if 0:
    #     alpha = np.array([rad for rad in instance.get('viewpoint_residual')]) if vp else np.ones((labels.shape))*(-10.00)
    # else:
    #     alpha = np.array([getfrombins(cl,bins) for cl in instance.get('viewpoint')]) if vp else np.ones((labels.shape))*(-10.00)
    alpha = np.ones((labels.shape))*(-10.00)
    h = np.array([[h,g] for h,g in instance.get('pred_height')]) if height_training else np.array([-1,-1000]*labels.shape)


    final_labels = []
    final_boxes = []
    final_h = []
    new_labels = []
    new_boxes = []
    new_h = []

    for gt in gt_objs:
      print(gt.kind_name)
      gt_name = gt.kind_name
      gt_bbox = gt.xmin, gt.ymin, gt.xmax, gt.ymax
      gt_alpha = gt.alpha
      gt_lhw =  gt.length, gt.width, gt.height

      name_map = {'Car':0,'Pedestrian':1,'Cyclist':2}



      class_index = np.where(labels==name_map[gt.kind_name])[0]     

      bb0 = get_best(boxes[:,0],gt.xmin,5)
      bb1 = get_best(boxes[:,1],gt.ymin,5)
      bb2 = get_best(boxes[:,2],gt.xmax,5)
      bb3 = get_best(boxes[:,3],gt.ymax,5)
    
      # print(class_index)
      # print(bb0)
      # print(bb1)
      # print(bb2)
      # print(bb3)


      # new_labels = []
      # new_boxes = []
      # new_h = []
      j = 0
      for i in range(5):

        new_labels.append(labels[class_index[i]])
    
        new_h.append(h[class_index[i]])
        new_boxes.append( [bb0[i],bb1[i],bb2[i],bb3[i]])



        j = j+1
        if j == 5:
          break
      # final_labels.append((np.array(new_labels)))
      # final_h.append((np.array(new_h)))
      # final_boxes.append((np.array(new_boxes)))


    # labels_mat = np.array(final_labels)
    # boxes_mat = np.array(final_h)
    # h_mat = np.array(final_boxes)
        

    # print(new_labels)
    # print(new_h)
    # print(new_boxes)


    labels_mat = np.array(new_labels)
    boxes_mat = np.array(new_boxes)
    h_mat = np.array(new_h)
    print(labels_mat.shape)
    print(boxes_mat.shape)
    print(h_mat.shape)
    print(labels_mat)
    print(boxes_mat)
    print(h_mat)
      # bb0 = k_nearest(5, gt.xmin,boxes[:,0] )
      # bb1 = k_nearest(5, gt.ymin,boxes[:,1] )
      # bb2 = k_nearest(5, gt.xmax,boxes[:,2] )
      # bb3 = k_nearest(5, gt.ymax,boxes[:,3] )

      # temph = k_nearest(5, gt.ymax,h[:,0] )
      # print(bb0)
      # print(bb1)
      # print(bb2)
      # print(bb3)
      # for i, b in enumerate(boxes):
      #   tempb = b[0],b[1],b[2],b[3]
      #   temp_name = labels[i]
      #   temp_h = h[i]

      #   print(get_min(gt.xmin,b[0]))
   
        #iou_score = box_iou(tempb, tempbbox)
   







    # Image BV
    bv_image = cv2.imread(filename).astype(np.uint8)

    if height_training:
        bv_ground = None
    else:
        # Ground BV
        bv_ground = np.fromfile(os.path.join(im_path,'ground_'+filename[-10:].split('.png')[0]+'.txt'),sep=' ')
        bv_ground = bv_ground.reshape(bv_image.shape[0],bv_image.shape[1],1)
    
    # Calibration for 3D
 
    calib_file = os.path.join(kitti_calib_path.replace('/training',''),filename[-10:].split('.png')[0]+'.txt')

    # Refiner for 3D
    refiner = BirdviewDetectionRefiner(bv_image, bv_ground, bvres, velodyne_h, only_front)

    im_ann = []
    im_ann_obj = []
    if write:
        file_ann  = open(os.path.join(output_folder,filename[-10:].split('.png')[0]+'.txt'), 'w+')
    for k, box in enumerate(boxes_mat):
        lbl = catName(labels_mat[k],nclass)
        ann,obj3d,strAnn = prepareAnn(lbl,alpha[k],boxes_mat,score=scores[k],h=h_mat[k,0],z=h_mat[k,1])

        if hwrot and height_training:
            refiner.refine_detection_rotated_wheight(obj3d)
        elif hwrot:
            refiner.refine_detection_rotated(obj3d)
        else:
            refiner.refine_detection(obj3d)
        if obj3d.height == -1:
            continue

        # Project points to camera frame coordinates
        calib = Calibration(calib_file)
        p = calib.project_velo_to_rect(np.array([[obj3d.location.x,obj3d.location.y,obj3d.location.z]]))

        # Change 2D bbox in BV getting 2D bbox in camera frame (projection)
        _,_,bbox2D = _draw_projection_obstacle_to_cam(obj3d, calib_file, bvres, only_front, False)
        if bbox2D == None:
            continue
        # Obtain alpha from yaw
        obj3d.alpha = obj3d.yaw -(-math.atan2(p[0][2],p[0][0]) - 1.5*math.pi)
        obj3d.alpha = obj3d.alpha%(2*math.pi)
        if obj3d.alpha > math.pi:
            obj3d.alpha -= 2*math.pi
        elif obj3d.alpha < -math.pi:
            obj3d.alpha += 2*math.pi

        # After refinement
        ann = [
               obj3d.kind_name, 
               obj3d.truncated,
               obj3d.occluded,
               round(obj3d.alpha,6),
               round(bbox2D[0],6),round(bbox2D[1],6),round(bbox2D[2],6),round(bbox2D[3],6),
               round(obj3d.height,6), round(obj3d.width,6), round(obj3d.length,6), 
               round(p[0][0],6), round(p[0][1],6), round(p[0][2],6), # Camera coordinates
               round(obj3d.yaw,6),
               obj3d.score, # DON'T ROUND IT
            ]

        im_ann.append(ann)
        im_ann_obj.append(obj3d)
        strAnn = ' '.join([str(x) for x in ann])
        
        if write:
            file_ann.write(strAnn+'\n')
    if write:
        file_ann.close()
    # print(instance)
    return  im_ann, im_ann_obj, instance

def main(config_file, ann_val, write, img2show, save_img, eval_chkp, force_test, score_thresh , nms_thresh, kitti_root ):
    # KITTI paths
    kitti_im_path = kitti_root+'/image_2'
    kitti_calib_path = kitti_root+'/calib'

    # LOGGER AND CONFIGURATION LOAD
    logger = logging.getLogger("detectron2.trainer")
    cfg = get_cfg()
    cfg.set_new_allowed(True)
    add_diffusiondet_config(cfg)
    add_model_ema_configs(cfg)
    cfg.DATASETS.TEST = ("kitti_train",)
    cfg.OUTPUT_DIR = '/content/output_final'
    cfg.merge_from_file(os.path.join(detectron2_root,"configs/{}.yaml".format(config_file)))
    # cfg.freeze()
    default_setup(cfg, None)

    nclasses = cfg.MODEL.ROI_HEADS.NUM_CLASSES
    optional_arguments = []
    if cfg.VIEWPOINT:
        optional_arguments.append('viewpoint')
    if cfg.VIEWPOINT_RESIDUAL:
        optional_arguments.append('vp_res')
    if cfg.ROTATED_BOX_TRAINING:
        optional_arguments.append('bbox3D')
    if 1:
        optional_arguments.append('height')

    val_path = detectron2_root+"/datasets/bv_kitti/annotations/{}.json".format(ann_val)
    register_coco_instances("kitti_train", {}, val_path, detectron2_root+'/datasets/bv_kitti/image', extra_arguments=optional_arguments)
    calib_root_path = '/content/DiffusionDetLidar/datasets/bv_kitti/label'


    toeval = []
    models = os.listdir(cfg.OUTPUT_DIR)
    for model in models:
        if model.endswith('.pth') and not model=='model_final.pth':
            toeval.append(model)
    toeval.sort()
    toeval = toeval[:-1]
    if force_test:
        toeval = [e for e in toeval if force_test in e]
        f_eval = [folder.split('_')[1].split('.')[0] for folder in toeval]
    elif eval_chkp!=0:
        length = len(toeval)//2
        toeval = toeval[length::eval_chkp]
        toeval.append('model_final.pth')
        f_eval = [folder.split('_')[1].split('.')[0] for folder in toeval]
    else:
        toeval = ['model_final.pth']
        f_eval = ['final']
    print('Checkpoints to be evaluated: ',toeval)
    for checkpoint, eval_folder in zip(toeval,f_eval):
        cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, checkpoint) 

        cfg.MODEL.DiffusionDet.L1_HEIGHT_WEIGHT= (5.0, 0.5, 10.0)
        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = score_thresh 
        cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = nms_thresh
        predictor = DefaultPredictor(cfg)
        

        val_bv_dicts = DatasetCatalog.get("kitti_train")
        val_bv_meta = MetadataCatalog.get("kitti_train")

        obj_anns = []
        kitti_results = []
        c = 0

        sample_idx = range(img2show) if img2show != 0 else [-1]
        # print('sample indx: ',sample_idx)
        logger.info("Showing {} predictions".format(str(img2show)))
        ann_outdir = os.path.join(cfg.OUTPUT_DIR,'annotations',eval_folder)
        if not os.path.exists(ann_outdir):
            os.makedirs(ann_outdir)

        for image_id, d in enumerate(val_bv_dicts):
            print('for ground truth images: ', image_id)
            c += 1
            file = os.path.join(ann_outdir,d["file_name"][-10:].split('.png')[0]+'.txt')
            print('pred path: ',file)
            im = cv2.imread(d["file_name"])
            print(im.shape)
            print("Preparing prediction {}, from {}, image: {}".format(str(c),str(len(val_bv_dicts)),d["file_name"]))

            gt_label_path = os.path.join(calib_root_path,d["file_name"][-10:].split('.png')[0]+'.txt')
            print( 'gt label: ',gt_label_path)
            f = open(gt_label_path)
            gt_labels = f.readlines()

            f.close()

            gt_objs = []

            for x in gt_labels:
              print('raw label: ' ,x)
              if x.split(' ')[0] ==  'Pedestrian' or x.split(' ')[0] == 'Cyclist' or x.split(' ')[0] == 'Car':
                temp = Object3d(x)
                temp.print_object
                gt_objs.append(temp)
            print('ground truth: ')
            for gt in gt_objs:
              gt.yaw = -10
              print('name: ',gt.kind_name,' trunc: ',gt.truncated,' occ: ',gt.occluded,' alpha: ',gt.alpha,' xmin: ',gt.xmin,' ymin: ',gt.ymin,' xmax: ',gt.xmax,' ymax: ',gt.ymax,' height: ',gt.height,' width: ',gt.width,' length: ',gt.length,' yaw: ',gt.yaw)





            # print(gt_label_path)
            # print(file_)

            #gt_objs = 
            # print('ground truth: ',d)

            if not os.path.exists(file) or write:
                is_kitti_ann=False
                # Inference
                outputs = predictor(im)
     
                # print('outputs:',outputs["instances"][:5])

                list_anns, obj_anns, instances = prepare_for_coco_detection_KITTI(outputs["instances"].to("cpu"), ann_outdir, d["file_name"], write, kitti_calib_path, nclasses, cfg.VIEWPOINT, cfg.VP_BINS, cfg.VIEWPOINT_RESIDUAL, cfg.ROTATED_BOX_TRAINING, cfg.HEIGHT_TRAINING,gt_objs)




                fixed_list_anns = []
                for idx,item in enumerate(list_anns):
                  # print(item)
                  if item[0] == 'Van':
                    list_anns[idx][0] = 'Pedestrian'
                    fixed_list_anns.append(list_anns[idx])
                  elif item[0] == 'Truck':
                    list_anns[idx][0] = 'Cyclist'
                    fixed_list_anns.append(list_anns[idx])
                  elif item[0] == 'Car':
                    fixed_list_anns.append(list_anns[idx])

                for obj in obj_anns:
                  if obj.kind_name == 'Van':
                    obj.kind_name = 'Pedestrian'
           
                  elif obj.kind_name == 'Truck':
                    obj.kind_name = 'Cyclist'
                # print(fixed_list_anns[:5])    


                #kitti_results.append(list_anns)
                kitti_results.append(fixed_list_anns)
            else:
                print('why we here')
                is_kitti_ann=True
                with open(file,'r') as f:
                    list_anns = f.read().splitlines()
                kitti_results.append([anns.split(' ') for anns in list_anns] if list_anns else [])
                for ann in list_anns:
                    obj_anns.append(Object3d(ann))
            # res_list = []
            # obj_anns_final = []





            # for gt_ in gt_objs:
            #   res_list = []
            #   for obj in obj_anns:


            #     res = obj.get_diff(gt_)
            #     print(res)
            #     res_list.append(res)
              
            #   res_mat = np.array(res_list)
            #   res_mat[res_mat<=0] = 'inf'
            #   print('result_mat: ',res_mat.shape)

            #   best = np.argmin(res_mat[1:11],axis=0)
            #   print('best index:',best)








            #   temp = (str(obj_anns[0].kind_name)+' ' + str(obj_anns[0].truncated) +' '+str(obj_anns[0].occluded) +' '+str(obj_anns[0].alpha)+' ' +str(obj_anns[best[4]].data[4])+' ' +str(obj_anns[best[5]].data[5])+' ' +
            #   str(obj_anns[best[6]].data[6])+' '+str(obj_anns[best[7]].data[7]) +' '+str(obj_anns[best[8]].data[8]) +' '+str(obj_anns[best[9]].data[9]) +' '+str(obj_anns[best[10]].data[10])+' ' +str(obj_anns[best[11]].data[11])+' ' +str(obj_anns[best[11]].data[12]))
            #   print(temp)
            #   obj_anns_final.append(res_mat[best])
            #   print('best index:',res_mat[best,np.array([0])])
   

            # print('c value: ', c)
            if c in sample_idx:
                # print('c in smaple idx')
                # Change BV aspect
                nonzero = np.where(im>0)
                im[nonzero]=255-im[nonzero]
                im=cv2.bitwise_not(im)

                kitti_im = cv2.imread(os.path.join(kitti_im_path,d["file_name"][-10:]))
                calib_file = os.path.join(kitti_calib_path.replace('/training',''),d["file_name"][-10:].split('.png')[0]+'.txt')
                # Show obstacles



                for i, obj in enumerate(obj_anns):
                  
                  if i == 10:
                    break
                  print('pred: ')
                  print('name: ',obj.kind_name,' trunc: ',obj.truncated,' occ: ',obj.occluded,' alpha: ',obj.alpha,' xmin: ',obj.xmin,' ymin: ',obj.ymin,' xmax: ',obj.xmax,' ymax: ',obj.ymax,' height: ',obj.height,' width: ',obj.width,' length: ',obj.length,' yaw: ',obj.yaw)

                
                  kitti_im, im, _ = _draw_projection_obstacle_to_cam(obj, calib_file, bvres, only_front, True, kitti_im, im, is_kitti_ann=is_kitti_ann)
                  # cv2.imshow('image',kitti_im)
                  # cv2.imshow('bv_image',im)
                  if save_img:
                      im_outdir = os.path.join(cfg.OUTPUT_DIR,'images')
                      if not os.path.exists(im_outdir):
                          os.makedirs(im_outdir)
                      cv2.imwrite(os.path.join(im_outdir,'3D_'+d["file_name"][-10:]), kitti_im)
                      cv2.imwrite(os.path.join(im_outdir,'BEV_'+d["file_name"][-10:]), im)
                  # cv2.waitKey(0)
                  # cv2.destroyAllWindows()
            elif c > max(sample_idx) and not write:
                print('c value bigger than max index: ',c)
                break

if __name__ == '__main__':
    args = parse_args()

    main(args.config_file, args.ann_val, args.write, args.img2show, args.save_img, args.eval_chkp, args.force_test, args.score, args.nms, args.kitti_root)